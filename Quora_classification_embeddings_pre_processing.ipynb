{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quora_classification_embeddings_pre-processing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CKDlRYXisl_",
        "colab_type": "text"
      },
      "source": [
        "# Text classification for insincere Quora questions\n",
        "(inspired by: https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings)\n",
        "\n",
        "Step 1 - pre-processing; the point here is to not use standard pre-processing steps but instead make sure that there is as much overlap between the word embeddings and your vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJexyWvh02go",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###imports\n",
        "import pandas as pd\n",
        "import re\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RirSOJanRsa",
        "colab_type": "code",
        "outputId": "92fe09a8-260a-412e-91a2-136afd0f2874",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "###mount drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "###change directory\n",
        "os.chdir('/content/gdrive/My Drive/Colab Notebooks/quora')\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqVpu6Bjjd9C",
        "colab_type": "code",
        "outputId": "6b1e16ea-168a-417f-9d8a-d52373a1df55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "###Data set explore\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "train.iloc[0:10]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qid</th>\n",
              "      <th>question_text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00002165364db923c7e6</td>\n",
              "      <td>How did Quebec nationalists see their province...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000032939017120e6e44</td>\n",
              "      <td>Do you have an adopted dog, how would you enco...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0000412ca6e4628ce2cf</td>\n",
              "      <td>Why does velocity affect time? Does velocity a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>000042bf85aa498cd78e</td>\n",
              "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0000455dfa3e01eae3af</td>\n",
              "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>00004f9a462a357c33be</td>\n",
              "      <td>Is Gaza slowly becoming Auschwitz, Dachau or T...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>00005059a06ee19e11ad</td>\n",
              "      <td>Why does Quora automatically ban conservative ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0000559f875832745e2e</td>\n",
              "      <td>Is it crazy if I wash or wipe my groceries off...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>00005bd3426b2d0c8305</td>\n",
              "      <td>Is there such a thing as dressing moderately, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>00006e6928c5df60eacb</td>\n",
              "      <td>Is it just me or have you ever been in this ph...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    qid  ... target\n",
              "0  00002165364db923c7e6  ...      0\n",
              "1  000032939017120e6e44  ...      0\n",
              "2  0000412ca6e4628ce2cf  ...      0\n",
              "3  000042bf85aa498cd78e  ...      0\n",
              "4  0000455dfa3e01eae3af  ...      0\n",
              "5  00004f9a462a357c33be  ...      0\n",
              "6  00005059a06ee19e11ad  ...      0\n",
              "7  0000559f875832745e2e  ...      0\n",
              "8  00005bd3426b2d0c8305  ...      0\n",
              "9  00006e6928c5df60eacb  ...      0\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd2YzbBLuBY0",
        "colab_type": "text"
      },
      "source": [
        "The below function builds the training vocabulary dictionary, going through all the sentences and counts the occurances of the contained words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HLMv2p9oLDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###build vocab dictionary function\n",
        "def build_vocab(sentences, verbose =  True):\n",
        "    \"\"\"\n",
        "    :param sentences: list of list of words\n",
        "    :return: dictionary of words and their count\n",
        "    \"\"\"\n",
        "    vocab = {}\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            try:\n",
        "                vocab[word] += 1\n",
        "            except KeyError:\n",
        "                vocab[word] = 1\n",
        "    return vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82oxs333oOgm",
        "colab_type": "code",
        "outputId": "a744d6eb-8577-4ff0-aa9f-83f52533059c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "###build vocab\n",
        "#split sentences into separate words\n",
        "sentences = train[\"question_text\"].apply(lambda x: x.split()).values\n",
        "#run vocab function\n",
        "vocab = build_vocab(sentences)\n",
        "#print first 5 elements of dictionary\n",
        "print({k: vocab[k] for k in list(vocab)[:5]})"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'How': 261930, 'did': 33489, 'Quebec': 97, 'nationalists': 91, 'see': 9003}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDTwYWDvoQwk",
        "colab_type": "code",
        "outputId": "8165ff17-108e-4056-eb07-0e8474041347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "###import google news embeddings\n",
        "from gensim.models import KeyedVectors\n",
        "#change directory\n",
        "os.chdir('/content/gdrive/My Drive/Colab Notebooks/album_reviews')\n",
        "news_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
        "#\n",
        "embeddings_index = KeyedVectors.load_word2vec_format(news_path, binary=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mca9EVnoZN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###a function to check the intersection between bocab and embeddings\n",
        "import operator \n",
        "\n",
        "def check_coverage(vocab,embeddings_index):\n",
        "    a = {}\n",
        "    oov = {}\n",
        "    k = 0\n",
        "    i = 0\n",
        "    for word in vocab:\n",
        "        #try to assign word from embedding to new dict with index value\n",
        "        #add number of found words to k\n",
        "        try:\n",
        "            a[word] = embeddings_index[word]\n",
        "            k += vocab[word]\n",
        "        #otherwise add word count value to oov dict word key\n",
        "        #add number of unfound words to i\n",
        "        except:\n",
        "            oov[word] = vocab[word]\n",
        "            i += vocab[word]\n",
        "            pass\n",
        "\n",
        "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
        "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
        "\n",
        "    return sorted_x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN8ulhD1NnJw",
        "colab_type": "text"
      },
      "source": [
        "We can see from the below results that 78.5% of our vocabulary is covered by word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFjw9W3coq_E",
        "colab_type": "code",
        "outputId": "fa2357d0-3ce4-41a2-b16f-4ff18e035352",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "###run vocab function\n",
        "oov = check_coverage(vocab,embeddings_index)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found embeddings for  78.75% of all text\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_5zdDeJNx6f",
        "colab_type": "text"
      },
      "source": [
        "To understand which words we print the top 10 out of vocabulary words, which show that stop words and punctuation are to blame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zY4K5sLIi-C",
        "colab_type": "code",
        "outputId": "755499f5-4560-45c3-867b-7ed46470ef0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "oov[:10]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('to', 403183),\n",
              " ('a', 402682),\n",
              " ('of', 330825),\n",
              " ('and', 251973),\n",
              " ('India?', 16384),\n",
              " ('it?', 12900),\n",
              " ('do?', 8753),\n",
              " ('life?', 7753),\n",
              " ('you?', 6295),\n",
              " ('me?', 6202)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOkAiF6KIeQ_",
        "colab_type": "text"
      },
      "source": [
        "## Pre-processing steps to remove words not covered by embeddings\n",
        "1.   Remove punctuation not in embeddings\n",
        "2.   Change numbers of 2 or more digits to hashes\n",
        "3.   Replace common misspellings, and american/british conflicts\n",
        "4.   Remove most common stop words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxjp2S4fDZhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###PP step 1 function\n",
        "def clean_text(x):\n",
        "\n",
        "    x = str(x)\n",
        "    for punct in \"/-'\":\n",
        "        x = x.replace(punct, ' ')\n",
        "    for punct in '&':\n",
        "        x = x.replace(punct, f' {punct} ')\n",
        "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
        "        x = x.replace(punct, '')\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zthY1K-bL3rz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###PP step 2 function\n",
        "def clean_numbers(x):\n",
        "\n",
        "    x = re.sub('[0-9]{5,}', '#####', x)\n",
        "    x = re.sub('[0-9]{4}', '####', x)\n",
        "    x = re.sub('[0-9]{3}', '###', x)\n",
        "    x = re.sub('[0-9]{2}', '##', x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2-sLCzkR13I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###PP step 3 function\n",
        "def _get_mispell(mispell_dict):\n",
        "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
        "    return mispell_dict, mispell_re\n",
        "\n",
        "\n",
        "mispell_dict = {'colour':'color',\n",
        "                'centre':'center',\n",
        "                'didnt':'did not',\n",
        "                'doesnt':'does not',\n",
        "                'isnt':'is not',\n",
        "                'shouldnt':'should not',\n",
        "                'favourite':'favorite',\n",
        "                'travelling':'traveling',\n",
        "                'counselling':'counseling',\n",
        "                'theatre':'theater',\n",
        "                'cancelled':'canceled',\n",
        "                'labour':'labor',\n",
        "                'organisation':'organization',\n",
        "                'wwii':'world war 2',\n",
        "                'citicise':'criticize',\n",
        "                'instagram': 'social medium',\n",
        "                'whatsapp': 'social medium',\n",
        "                'snapchat': 'social medium'\n",
        "\n",
        "                }\n",
        "mispellings, mispellings_re = _get_mispell(mispell_dict)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDAut3OPTC0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###PP step 3 function\n",
        "def replace_typical_misspell(text):\n",
        "    def replace(match):\n",
        "        return mispellings[match.group(0)]\n",
        "\n",
        "    return mispellings_re.sub(replace, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUKtJNw3K4dA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train[\"question_text\"] = train[\"question_text\"].apply(lambda x: clean_text(x))\n",
        "sentences = train[\"question_text\"].apply(lambda x: x.split())\n",
        "vocab = build_vocab(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5lXbJydLpVS",
        "colab_type": "code",
        "outputId": "e0ee56cb-0499-4608-969f-415eab3e30ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "oov = check_coverage(vocab,embeddings_index)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found embeddings for  89.99% of all text\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Lp1pdqpOg8-",
        "colab_type": "text"
      },
      "source": [
        "The results above show that PP step 1 significantly improved vocabulary coverage to 89.99%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLtcADWSMRPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train[\"question_text\"] = train[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
        "sentences = train[\"question_text\"].apply(lambda x: x.split())\n",
        "vocab = build_vocab(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e93erB4ZMWaB",
        "colab_type": "code",
        "outputId": "2e5b4348-7f5d-42a2-d8e2-25451b1f9337",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "oov = check_coverage(vocab,embeddings_index)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found embeddings for  90.75% of all text\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojn_iQkzQfWE",
        "colab_type": "text"
      },
      "source": [
        "The results above show that PP step 2 improved vocabulary coverage to 90.75%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzDN6fSnRAtw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train[\"question_text\"] = train[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n",
        "sentences = train[\"question_text\"].apply(lambda x: x.split())\n",
        "vocab = build_vocab(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG2qzCVYTXVw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ddf89dbf-cc7d-49cf-b1c9-377ed599f3ae"
      },
      "source": [
        "oov = check_coverage(vocab,embeddings_index)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found embeddings for  90.81% of all text\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP0iKlV_TYAI",
        "colab_type": "text"
      },
      "source": [
        "The results above show that PP step 3 improved vocabulary coverage to 90.81%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrDkDOkETtBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "to_remove = ['a','to','of','and']\n",
        "sentences = [[word for word in sentence if not word in to_remove] for sentence in sentences]\n",
        "vocab = build_vocab(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5ffp99NT0RL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c6e9fcd9-4376-45b6-e2d0-580204628f5e"
      },
      "source": [
        "oov = check_coverage(vocab,embeddings_index)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found embeddings for  98.96% of all text\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVrRHCp6T1wy",
        "colab_type": "text"
      },
      "source": [
        "The results above show that PP step 4 improved vocabulary coverage to 98.96%. This is the final step of our pre-processing, from the word below we can see that there is no easy way to improve coverage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96v5V_hcUGd8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "9d4ddeca-7c75-4c02-d04e-a6262b0356c2"
      },
      "source": [
        "oov[:10]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('bitcoin', 987),\n",
              " ('Quorans', 858),\n",
              " ('cryptocurrency', 822),\n",
              " ('Snapchat', 807),\n",
              " ('btech', 632),\n",
              " ('Brexit', 493),\n",
              " ('cryptocurrencies', 481),\n",
              " ('blockchain', 474),\n",
              " ('behaviour', 468),\n",
              " ('upvotes', 432)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkKE-KGAg1Ic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "os.chdir('/content/gdrive/My Drive/Colab Notebooks/quora')\n",
        "train[\"question_text\"] = sentences\n",
        "\n",
        "with open('sentences_pp', 'wb') as fp:\n",
        "    pickle.dump(train, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}