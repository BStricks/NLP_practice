{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow_2.0_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BStricks/NLP_practice/blob/master/tensorflow_2_0_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJJkfRI3KwDi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0464d44e-bab8-46b8-e7cd-a98d312c918b"
      },
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.0.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.15.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/freeze_graph\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-1.15.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow_core/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-1.15.0\n",
            "Collecting tensorflow==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 118kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.1.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.15.0)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 41.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.17.4)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 37.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.33.6)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0) (42.0.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.8.0)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/f8/84b5771faec3eba9fe0c91c8c5896364a8ba08852c0dea5ad2025026dd95/google_auth-1.10.0-py2.py3-none-any.whl (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.21.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.16.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.7)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.0)\n",
            "\u001b[31mERROR: tensorboard 2.0.2 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.10.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, google-auth, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: google-auth 1.4.2\n",
            "    Uninstalling google-auth-1.4.2:\n",
            "      Successfully uninstalled google-auth-1.4.2\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "Successfully installed google-auth-1.10.0 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_core",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpdD-n61MyVd",
        "colab_type": "code",
        "outputId": "426aa7a6-229a-449f-8751-8e49a00faae5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aldF7b6vNzfm",
        "colab_type": "text"
      },
      "source": [
        "## **Step 1: define the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqr-yo4mNqjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGX3I191OmA_",
        "colab_type": "text"
      },
      "source": [
        "## **Step 2: compile the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8nIAuuIOtll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Select an algorithm to perform the optimisation procedure:\n",
        "\n",
        "Adadelta: a stochastic gradient descent method that is based on adaptive \n",
        "learning rate per dimension to address two drawbacks: 1) the continual decay of \n",
        "learning rates throughout training 2) the need for a manually selected global \n",
        "learning rate\n",
        "Adagrad: Adagrad is an optimizer with parameter-specific learning rates, \n",
        "which are adapted relative to how frequently a parameter gets updated during \n",
        "training. The more updates a parameter receives, the smaller the updates.\n",
        "Adam: Adam optimization is a stochastic gradient descent method that is based \n",
        "on adaptive estimation of first-order and second-order moments. \n",
        "Adamax: It is a variant of Adam based on the infinity norm. Default parameters \n",
        "follow those provided in the paper. Adamax is sometimes superior to adam, \n",
        "specially in models with embeddings.\n",
        "Ftrl: \n",
        "Nadam: Much like Adam is essentially RMSprop with momentum, Nadam is Adam with \n",
        "Nesterov momentum.\n",
        "Optimizer: \n",
        "RMSprop: \n",
        "SGD: Stochastic gradient descent and momentum optimizer.\n",
        "\n",
        "Select a loss function to use with the optimizer:\n",
        "\n",
        "BinaryCrossentropy: Computes the cross-entropy loss between true labels and predicted labels.\n",
        "CategoricalCrossentropy: Computes the crossentropy* loss between the labels and predictions.\n",
        "CategoricalHinge: Computes the categorical hinge* loss between y_true and y_pred.\n",
        "CosineSimilarity: Computes the cosine similarity between y_true and y_pred.\n",
        "Hinge*:\n",
        "Huber: Computes the Huber loss between y_true and y_pred.\n",
        "KLDivergence: Computes Kullback-Leibler divergence loss between y_true and y_pred\n",
        "LogCosh: Computes the logarithm of the hyperbolic cosine of the prediction error.\n",
        "Loss: Loss base class.\n",
        "MeanAbsoluteError: Computes the mean of absolute difference between labels and predictions.\n",
        "MeanAbsolutePercentageError: Computes the mean absolute percentage error between y_true and y_pred.\n",
        "MeanSquaredError: Computes the mean of squares of errors between labels and predictions.\n",
        "MeanSquaredLogarithmicError: Computes the mean squared logarithmic error between y_true and y_pred.\n",
        "Poisson: Computes the Poisson loss between y_true and y_pred.\n",
        "Reduction: Types of loss reduction.\n",
        "SparseCategoricalCrossentropy*: Computes the crossentropy loss between the labels and predictions.\n",
        "SquaredHinge*: Computes the squared hinge loss between y_true and y_pred.\n",
        "*The main difference between the hinge loss and the cross entropy loss is that the former \n",
        "arises from trying to maximize the margin between our decision boundary and data points - \n",
        "thus attempting to ensure that each point is correctly and confidently classified, \n",
        "while the latter comes from a maximum likelihood estimate of our model’s parameters. \n",
        "The softmax function, whose scores are used by the cross entropy loss, allows us to \n",
        "interpret our model’s scores as relative probabilities against each other.\n",
        "\n",
        "Select a metric to monitor the algorithm with during training:\n",
        "\n",
        "AUC: Computes the approximate AUC (Area under the curve) via a Riemann sum.\n",
        "Accuracy: Calculates how often predictions matches labels.\n",
        "BinaryAccuracy: Calculates how often predictions matches labels.\n",
        "BinaryCrossentropy: Computes the crossentropy metric between the labels and predictions.\n",
        "CategoricalAccuracy: Calculates how often predictions matches labels.\n",
        "CategoricalCrossentropy: Computes the crossentropy metric between the labels and predictions.\n",
        "CategoricalHinge: Computes the categorical hinge metric between y_true and y_pred.\n",
        "CosineSimilarity: Computes the cosine similarity between the labels and predictions.\n",
        "FalseNegatives: Calculates the number of false negatives.\n",
        "FalsePositives: Calculates the number of false positives.\n",
        "Hinge: Computes the hinge metric between y_true and y_pred.\n",
        "KLDivergence: Computes Kullback-Leibler divergence metric between y_true and y_pred.\n",
        "LogCoshError: Computes the logarithm of the hyperbolic cosine of the prediction error.\n",
        "Mean: Computes the (weighted) mean of the given values.\n",
        "MeanAbsoluteError: Computes the mean absolute error between the labels and predictions.\n",
        "MeanAbsolutePercentageError: Computes the mean absolute percentage error between y_true and y_pred.\n",
        "MeanIoU: Computes the mean Intersection-Over-Union metric.\n",
        "MeanRelativeError: Computes the mean relative error by normalizing with the given values.\n",
        "MeanSquaredError: Computes the mean squared error between y_true and y_pred.\n",
        "MeanSquaredLogarithmicError: Computes the mean squared logarithmic error between y_true and y_pred.\n",
        "MeanTensor: Computes the element-wise (weighted) mean of the given tensors.\n",
        "Metric: Encapsulates metric logic and state.\n",
        "Poisson: Computes the Poisson metric between y_true and y_pred.\n",
        "Precision: Computes the precision of the predictions with respect to the labels.\n",
        "Recall: Computes the recall of the predictions with respect to the labels.\n",
        "RootMeanSquaredError: Computes root mean squared error metric between y_true and y_pred.\n",
        "SensitivityAtSpecificity: Computes the sensitivity at a given specificity.\n",
        "SparseCategoricalAccuracy: Calculates how often predictions matches integer labels.\n",
        "SparseCategoricalCrossentropy: Computes the crossentropy metric between the labels and predictions.\n",
        "SparseTopKCategoricalAccuracy: Computes how often integer targets are in the top K predictions.\n",
        "SpecificityAtSensitivity: Computes the specificity at a given sensitivity.\n",
        "SquaredHinge: Computes the squared hinge metric between y_true and y_pred.\n",
        "Sum: Computes the (weighted) sum of the given values.\n",
        "TopKCategoricalAccuracy: Computes how often targets are in the top K predictions.\n",
        "TrueNegatives: Calculates the number of true negatives.\n",
        "TruePositives: Calculates the number of true positives.\n",
        "\"\"\"\n",
        "opt = SGD(learning_rate=0.01, momentum=0.9)\n",
        "model.compile(optimizer=opt, loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiBsWsnRQfeO",
        "colab_type": "text"
      },
      "source": [
        "## **Step 3: fit the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnyZHKr6TRub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Neural networks are trained using gradient descent where the estimate of the error \n",
        "used to update the weights is calculated based on a subset of the training dataset.\n",
        "\n",
        "The number of examples from the training dataset used in the estimate of the error \n",
        "gradient is called the batch size and is an important hyperparameter that influences \n",
        "the dynamics of the learning algorithm.\n",
        "\n",
        "1) Batch size controls the accuracy of the estimate of the error gradient when training neural networks.\n",
        "2) Batch, Stochastic, and Minibatch gradient descent are the three main flavors of the learning algorithm.\n",
        "3) There is a tension between batch size and the speed and stability of the learning process.\n",
        "\"\"\"\n",
        "model.fit(X, y, epochs=100, batch_size=32, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBJQLM_UT8Lo",
        "colab_type": "text"
      },
      "source": [
        "## **Step 4: evaluate the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOQkGKv2UDcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "This should be data not used in the training process so that we can get an \n",
        "unbiased estimate of the performance of the model when making predictions \n",
        "on new data.\n",
        "\"\"\"\n",
        "#where x and y are a hold out set\n",
        "loss = model.evaluate(X, y, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-s1CIh-UfO4",
        "colab_type": "text"
      },
      "source": [
        "## **Method 1 example: sequential model building API**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihLU9jjmW_0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "# define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(100, input_shape=(8,)))\n",
        "model.add(Dense(80))\n",
        "model.add(Dense(30))\n",
        "model.add(Dense(10))\n",
        "model.add(Dense(5))\n",
        "model.add(Dense(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzSMHFJvUk93",
        "colab_type": "text"
      },
      "source": [
        "## **Method 2 example: functional model building API**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVtFbMZIXOny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "# define the layers\n",
        "x_in = Input(shape=(8,))\n",
        "x = Dense(10)(x_in)\n",
        "x_out = Dense(1)(x)\n",
        "# define the model\n",
        "model = Model(inputs=x_in, outputs=x_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbMLZL4wYd8A",
        "colab_type": "text"
      },
      "source": [
        "# **Deep Learning model examples: MLP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRQuCdn7aVt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This dataset involves predicting whether a structure is in the atmosphere given radar returns.\n",
        "\"\"\"\n",
        "MLP - a standard fully connected neural network model.\n",
        "\n",
        "It is comprised of layers of nodes where each node is connected to all outputs \n",
        "from the previous layer and the output of each node is connected to all inputs for \n",
        "nodes in the next layer.\n",
        "\n",
        "An MLP is created by with one or more Dense layers. This model is appropriate for \n",
        "tabular data, that is data as it looks in a table or spreadsheet with one column \n",
        "for each variable and one row for each variable. There are three predictive modeling \n",
        "problems you may want to explore with an MLP; they are binary classification, \n",
        "multiclass classification, and regression.\n",
        "\"\"\"\n",
        "# mlp for binary classification\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUxWXOc1gz9E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd47beb9-15c9-4f97-8cbc-0aeff04fdc1f"
      },
      "source": [
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'\n",
        "df = read_csv(path, header=None)\n",
        "# split into input and output columns\n",
        "X, y = df.values[:, :-1], df.values[:, -1]\n",
        "# ensure all data are floating point values\n",
        "X = X.astype('float32')\n",
        "# encode strings to integer\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "# split into train and test datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "# determine the number of input features\n",
        "n_features = X_train.shape[1]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(235, 34) (116, 34) (235,) (116,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVx1MTR2hJiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define model\n",
        "\"\"\"\n",
        "activation function - \n",
        "kernel initializer - \n",
        "\"\"\"\n",
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
        "model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s4nKh4PhPLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHzBDvPUhReN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit the model\n",
        "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08ycoJ-MhT3m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "78188630-d35a-464b-b12f-70d648286448"
      },
      "source": [
        "# evaluate the model\n",
        "y_predictions = model.predict_classes(X_test)\n",
        "print(classification_report(y_test,y_predictions))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.80      0.88        41\n",
            "           1       0.90      0.99      0.94        75\n",
            "\n",
            "    accuracy                           0.92       116\n",
            "   macro avg       0.94      0.90      0.91       116\n",
            "weighted avg       0.93      0.92      0.92       116\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj31EfHxsA2O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c1a687e7-f637-4888-dcb7-8773b8e5f006"
      },
      "source": [
        "# load the dataset\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv'\n",
        "df = read_csv(path, header=None)\n",
        "# split into input and output columns\n",
        "X, y = df.values[:, :-1], df.values[:, -1]\n",
        "# ensure all data are floating point values\n",
        "X = X.astype('float32')\n",
        "# encode strings to integer\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "# split into train and test datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "# determine the number of input features\n",
        "n_features = X_train.shape[1]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 4) (50, 4) (100,) (50,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqMpAd3UsP9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
        "model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(3, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYmCBz4isajG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlBPQ6rHseZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit the model\n",
        "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHKqY33ZsikZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "17be3134-fe44-42e6-e549-6b17c5928c39"
      },
      "source": [
        "# evaluate the model\n",
        "y_predictions = model.predict_classes(X_test)\n",
        "print(classification_report(y_test,y_predictions))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        12\n",
            "           1       0.95      0.95      0.95        19\n",
            "           2       0.95      0.95      0.95        19\n",
            "\n",
            "    accuracy                           0.96        50\n",
            "   macro avg       0.96      0.96      0.96        50\n",
            "weighted avg       0.96      0.96      0.96        50\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6nbBuGVs9aE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fd1a4134-bd65-4999-f41f-700ee9ac9578"
      },
      "source": [
        "# mlp for regression\n",
        "from numpy import sqrt\n",
        "# load the dataset\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
        "df = read_csv(path, header=None)\n",
        "# split into input and output columns\n",
        "X, y = df.values[:, :-1], df.values[:, -1]\n",
        "# split into train and test datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "# determine the number of input features\n",
        "n_features = X_train.shape[1]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(339, 13) (167, 13) (339,) (167,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOZZVCChtHEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
        "model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQQTHY45tW-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='mse')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxrCnZuVtZh8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit the model\n",
        "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FauX9G5Ftbam",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8dda84dd-e35b-421e-ef0e-27368d1045c3"
      },
      "source": [
        "# evaluate the model\n",
        "error = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('MSE: %.3f, RMSE: %.3f' % (error, sqrt(error)))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE: 36.022, RMSE: 6.002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKQwV4wpYm2d",
        "colab_type": "text"
      },
      "source": [
        "# **Deep Learning model examples: CNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQdxBr5AaWwr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "26d91ebd-8cf7-438b-ee2f-33c6ec69dd7a"
      },
      "source": [
        "\"\"\"\n",
        "Convolutional Neural Networks - a type of network designed for image input, \n",
        "they are comprised of convolutional layers that extract features (called feature maps) \n",
        "and pooling layers that distill features down to the most salient elements.\n",
        "\"\"\"\n",
        "# example of a cnn for image classification\n",
        "from numpy import unique\n",
        "from numpy import argmax\n",
        "from tensorflow.keras.datasets.mnist import load_data\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "# load dataset\n",
        "(x_train, y_train), (x_test, y_test) = load_data()\n",
        "# reshape data to have a single channel\n",
        "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], x_train.shape[2], 1))\n",
        "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], x_test.shape[2], 1))\n",
        "# determine the shape of the input images\n",
        "in_shape = x_train.shape[1:]\n",
        "# determine the number of classes\n",
        "n_classes = len(unique(y_train))\n",
        "print(in_shape, n_classes)\n",
        "# normalize pixel values\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "(28, 28, 1) 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2krFvvDfXezn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), activation='relu', kernel_initializer='he_uniform', input_shape=in_shape))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(n_classes, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c18Zmq-QXncJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define loss and optimizer\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOrSXPNkXqwZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0bef1e47-4e9d-4992-f07e-f5fdae08d407"
      },
      "source": [
        "# fit the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=128, verbose=0)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f521f81f438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbVRJtTEXsv_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c4b27b6d-42de-4648-cdca-08fc6af5a35c"
      },
      "source": [
        "# evaluate the model\n",
        "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Accuracy: %.3f' % acc)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.987\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeKwoGLGYx_Z",
        "colab_type": "text"
      },
      "source": [
        "# **Deep Learning model examples: RNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mhr2UsiBaX4P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1273fdbb-62af-44e0-ae27-bd432a50c167"
      },
      "source": [
        "\"\"\"\n",
        "Recurrent Neural Networks, or RNNs for short, are designed to operate upon sequences of data.\n",
        "\"\"\"\n",
        "\n",
        "# lstm for time series forecasting\n",
        "from numpy import sqrt\n",
        "from numpy import asarray\n",
        "from pandas import read_csv\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        " \n",
        "# split a univariate sequence into samples\n",
        "def split_sequence(sequence, n_steps):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(len(sequence)):\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + n_steps\n",
        "\t\t# check if we are beyond the sequence\n",
        "\t\tif end_ix > len(sequence)-1:\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn asarray(X), asarray(y)\n",
        " \n",
        "# load the dataset\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-car-sales.csv'\n",
        "df = read_csv(path, header=0, index_col=0, squeeze=True)\n",
        "# retrieve the values\n",
        "values = df.values.astype('float32')\n",
        "# specify the window size\n",
        "n_steps = 5\n",
        "# split into samples\n",
        "X, y = split_sequence(values, n_steps)\n",
        "# reshape into [samples, timesteps, features]\n",
        "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "# split into train/test\n",
        "n_test = 12\n",
        "X_train, X_test, y_train, y_test = X[:-n_test], X[-n_test:], y[:-n_test], y[-n_test:]\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(91, 5, 1) (12, 5, 1) (91,) (12,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLAFz_nY9Gh6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, activation='relu', kernel_initializer='he_normal', input_shape=(n_steps,1)))\n",
        "model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2VBhXNN9KBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spY8MuYm9Pna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit the model\n",
        "model.fit(X_train, y_train, epochs=350, batch_size=32, verbose=2, validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcSMA33H9V4n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "afaef82d-db5f-46e2-941e-d6ce03193bed"
      },
      "source": [
        "# evaluate the model\n",
        "mse, mae = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('MSE: %.3f, RMSE: %.3f, MAE: %.3f' % (mse, sqrt(mse), mae))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE: 16946080.000, RMSE: 4116.562, MAE: 3303.697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYXkdWhX9sOf",
        "colab_type": "text"
      },
      "source": [
        "### **How to visualise**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDAXY-FCA1iH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daOE_e9GDoBv",
        "colab_type": "text"
      },
      "source": [
        "### **How to Reduce Overfitting With Dropout**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nrGwen5Ckhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Dropout is a clever regularization method that reduces overfitting of the training \n",
        "dataset and makes the model more robust. This is achieved during training, \n",
        "where some number of layer outputs are randomly ignored or “dropped out.” \n",
        "This has the effect of making the layer look like – and be treated like – a layer \n",
        "with a different number of nodes and connectivity to the prior layer.\n",
        "Dropout has the effect of making the training process noisy, forcing nodes \n",
        "within a layer to probabilistically take on more or less responsibility for the inputs.\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAG_A4YYEIfW",
        "colab_type": "text"
      },
      "source": [
        "### **How to Accelerate Training With Batch Normalization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92gP00Z_ELl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Batch normalization is a technique for training very deep neural networks that \n",
        "standardizes the inputs to a layer for each mini-batch. This has the effect of \n",
        "stabilizing the learning process and dramatically reducing the number of training \n",
        "epochs required to train deep networks.\n",
        "\"\"\"\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}